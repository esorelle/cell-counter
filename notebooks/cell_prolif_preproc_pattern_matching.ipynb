{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define image paths & some other items we'll need\n",
    "\n",
    "Note the OpenCV variables in all-caps. These are just helpful variable names for values OpenCV uses internally. This style of all-caps is commonly found in C programming for enumeration data types (constants), and is used to help make programs easier to read and maintain.\n",
    "\n",
    "See https://docs.opencv.org/4.2.0/d4/d86/group__imgproc__filter.html for more info on enumerations related to image filtering in OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You'll need to fix the path to your images here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../t01/'\n",
    "apt_ref_path = 'apt_ref.tif'\n",
    "fid_ref_path = 'fidicucial_ref.tif'\n",
    "img_paths = sorted(glob(os.path.join(data_dir, '*.tif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_ref = Image.open(fid_ref_path)\n",
    "fid_ref = np.asarray(fid_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "_ = plt.imshow(fid_ref, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig_refs = []\n",
    "\n",
    "for i in range(10):\n",
    "    dig_ref = Image.open('dig_ref_%d.tif' % i)\n",
    "    dig_ref = np.asarray(dig_ref)\n",
    "    dig_refs.append(dig_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10, sharey=True)\n",
    "\n",
    "for i, dig_ref in enumerate(dig_refs):\n",
    "    _ = axes[i].imshow(dig_ref, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_mask = Image.open(apt_ref_path)\n",
    "apt_ref_mask = np.asarray(apt_ref_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_c, _ = cv2.findContours(apt_ref_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(apt_ref_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_c = apt_ref_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mask = np.zeros(apt_ref_mask.shape, dtype=np.uint8)\n",
    "cv2.drawContours(c_mask, [apt_ref_c], 0, 255, 1)\n",
    "plt.figure(figsize=(8, 8))\n",
    "_ = plt.imshow(c_mask, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a test image\n",
    "\n",
    "Load the first image in the list using the PIL library (the only usage of PIL we will need). Once loaded, I check the shape and min/max values to determine the number of channels in the image and the range of values (8-bit vs 16-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = img_paths[0]\n",
    "# img_path = '../t01/BF_ST_035_APT_032_20190311094613.tif'\n",
    "print(os.path.basename(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(img_path)\n",
    "img = np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, img.max(), img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img.min(), img.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the file is a single channel grayscale image with 16-bit pixel values. I want to work with 8-bit pixel values, so we'll scale the values down. We could have simply cast the 16-bit array to 8-bit but these operations often will automatically normalize the min/max values in the data. We don't want to alter the data other than to scale it.\n",
    "\n",
    "First, we scale the 16-bit integers to an 8-bit range, but this creates floats. The floats are then cast to uint8.\n",
    "\n",
    "**I'm also flipping the image horizontally after the 8-bit conversion because I found the NumPy flip method altered the original 16-bit values (don't know why)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b = img / (2**8 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b.min(), img_8b.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b = img_8b.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b = cv2.flip(img_8b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b.min(), img_8b.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "_ = plt.imshow(img_8b, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cv2.matchTemplate(img_8b, fid_ref, cv2.TM_CCOEFF_NORMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "_ = plt.imshow(res > .7, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv2.findContours(\n",
    "    (res > .7).astype(np.uint8),\n",
    "    cv2.RETR_LIST,\n",
    "    cv2.CHAIN_APPROX_SIMPLE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_h, fid_w = fid_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_centers = []\n",
    "new_img = cv2.cvtColor(img_8b, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "for c in contours:\n",
    "    c_min_rect = cv2.minAreaRect(c)\n",
    "    loc = np.array(c_min_rect[0])\n",
    "    loc += (np.array(fid_ref.shape) / 2.) - 1\n",
    "    loc = np.round(loc).astype(np.uint)\n",
    "    \n",
    "    c_centers.append(loc)\n",
    "    \n",
    "    cv2.circle(new_img, tuple(loc), 5, (0, 255, 0), -1)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we tackle the image rotation\n",
    "\n",
    "This is done by assigning fiducials to a row, then finding the slope of each row. I took the mean of the slopes and then created a transformation matrix for that rotation angle and applied it to our base 8-bit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_y = [cnt[1] for cnt in c_centers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h = img_8b.shape[1]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.xlim(0, img_h)\n",
    "plt.xticks(range(0, img_h, 100))\n",
    "_ = plt.hist(centers_y, bins=int(np.sqrt(img_h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows are separated by roughly 220px\n",
    "assigned_idx = []\n",
    "centers_y = np.array(centers_y)\n",
    "row_dist = 20\n",
    "rows = []\n",
    "\n",
    "for i, cy in enumerate(centers_y):\n",
    "    if i in assigned_idx:\n",
    "        continue\n",
    "    \n",
    "    row_min = cy - row_dist\n",
    "    row_max = cy + row_dist\n",
    "    \n",
    "    in_row = np.logical_and(centers_y > row_min, centers_y < row_max)\n",
    "    row_membership = np.where(in_row)\n",
    "    row_members = list(row_membership[0])\n",
    "    print(row_members)\n",
    "    \n",
    "    rows.append(row_members)\n",
    "    assigned_idx.extend(row_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_centers = np.array(c_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_degs = []\n",
    "\n",
    "for r in rows:\n",
    "    gradient, intercept, r_value, p_value, std_err = stats.linregress(c_centers[r])\n",
    "    r_deg = np.degrees(np.arctan(gradient))\n",
    "    r_degs.append(r_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_degs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_deg_mean = np.mean(r_degs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_deg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = img_8b.shape\n",
    "\n",
    "rot_mat = cv2.getRotationMatrix2D((cols/2., rows/2.), r_deg_mean, 1)\n",
    "img_rot = cv2.warpAffine(img_8b, rot_mat, (cols, rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(img_rot, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I define a function to rotate a point around another point. This allows us to transform all the fiducial center locations to the rotated space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(point, origin=(0, 0), degrees=0):\n",
    "    angle = np.deg2rad(-degrees)\n",
    "    \n",
    "    ox, oy = origin\n",
    "    px, py = point\n",
    "\n",
    "    qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)\n",
    "    qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)\n",
    "    \n",
    "    return qx, qy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_c = rotate(c_centers[20], origin=(cols/2., rows/2.), degrees=r_deg_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_centers[20], rot_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rotate function works, so apply it to all fiducial centers. However, while we do this I calculate the bounding boxes for the regions of interest relative to the fiducials, e.g. the apartment row/col numbers. Additionally, I collect regions I wanted to use for applying the luminosity correction...but this didn't work well and the regions I selected are inside the apartment so wouldn't be ideal for later time points when they could be filled with cells :(  I have removed the uniformity correction code from here so you don't have to install my cv2-extras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_img = cv2.cvtColor(img_rot, cv2.COLOR_GRAY2RGB)\n",
    "row_text_regions = []\n",
    "col_text_regions = []\n",
    "uni_corr_regions = []\n",
    "apt_regions = []\n",
    "\n",
    "for c_center in c_centers:\n",
    "    rot_c = rotate(c_center, origin=(cols/2., rows/2.), degrees=r_deg_mean)\n",
    "    c_int_tup = tuple(np.round(rot_c).astype(np.int))\n",
    "    \n",
    "    if c_int_tup[0] < 150 or c_int_tup[1] < 130:\n",
    "        continue\n",
    "    \n",
    "    # rect for non-uniformity samples\n",
    "    rect_vert1 = (c_int_tup[0] - 80, c_int_tup[1] - 50)\n",
    "    rect_vert2 = (c_int_tup[0] - 30, c_int_tup[1])\n",
    "    \n",
    "    uni_corr_regions.append(\n",
    "        [\n",
    "            rect_vert1,\n",
    "            (c_int_tup[0] - 30, c_int_tup[1] - 50),\n",
    "            rect_vert2,\n",
    "            (c_int_tup[0] - 80, c_int_tup[1])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # rect for row number\n",
    "    row_rect_vert1 = (c_int_tup[0] - 10, c_int_tup[1] - 128)\n",
    "    row_rect_vert2 = (c_int_tup[0] + 41, c_int_tup[1] - 100)\n",
    "    \n",
    "    row_text_regions.append(\n",
    "        img_rot[c_int_tup[1] - 128:c_int_tup[1] - 100, c_int_tup[0] - 10:c_int_tup[0] + 41]\n",
    "    )\n",
    "    \n",
    "    # rect for col number\n",
    "    col_rect_vert1 = (c_int_tup[0] - 148, c_int_tup[1] - 29)\n",
    "    col_rect_vert2 = (c_int_tup[0] - 97, c_int_tup[1] - 1)\n",
    "        \n",
    "    col_text_regions.append(\n",
    "        img_rot[c_int_tup[1] - 29:c_int_tup[1] - 1, c_int_tup[0] - 148:c_int_tup[0] - 97]\n",
    "    )\n",
    "    \n",
    "    # apt region\n",
    "    apt_offset_x = c_int_tup[0] - apt_ref_mask.shape[1] - 10\n",
    "    apt_offset_y = c_int_tup[1] - apt_ref_mask.shape[0] + 45\n",
    "    apt_c = apt_ref_c + [apt_offset_x, apt_offset_y]\n",
    "    \n",
    "    cv2.circle(new_img, c_int_tup, 5, (0, 255, 0), -1)\n",
    "    #cv2.rectangle(new_img, rect_vert1, rect_vert2, (0, 255, 0), 1)\n",
    "    cv2.rectangle(new_img, row_rect_vert1, row_rect_vert2, (0, 255, 0), 1)\n",
    "    cv2.rectangle(new_img, col_rect_vert1, col_rect_vert2, (0, 255, 0), 1)\n",
    "    cv2.drawContours(new_img, [apt_c], 0, (0, 255, 0), 1)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img[300:800, 300:800], cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does pattern matching work with digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_0 = cv2.matchTemplate(img_8b, dig_refs[7], cv2.TM_CCOEFF_NORMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "_ = plt.imshow(res_0 > .8, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find apartment addresses using pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_digit(dig_region):\n",
    "    # padding is crucial & needs to be about 1/2  of the template width/height\n",
    "    dig_region_pad = np.pad(dig_region, 10, mode='median')\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for i, dig_ref in enumerate(dig_refs):\n",
    "        res = cv2.matchTemplate(dig_region_pad, dig_ref, cv2.TM_CCOEFF_NORMED)\n",
    "        scores.append(res.max())\n",
    "    \n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: the 3-digit row/col regions were slightly re-defined above to ensure divisibility by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_digits = []\n",
    "\n",
    "for r in row_text_regions:\n",
    "    r_split = np.split(r, 3, axis=1)\n",
    "    \n",
    "    digits = []\n",
    "    \n",
    "    for sub_r in r_split:\n",
    "        single_digits.append(sub_r)\n",
    "        \n",
    "        digits.append(str(identify_digit(sub_r)))\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(''.join(digits))\n",
    "    plt.imshow(r, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in col_text_regions:\n",
    "    r_split = np.split(r, 3, axis=1)\n",
    "    \n",
    "    digits = []\n",
    "    \n",
    "    for sub_r in r_split:        \n",
    "        digits.append(str(identify_digit(sub_r)))\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(''.join(digits))\n",
    "    plt.imshow(r, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(single_digits[2], cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cv2.matchTemplate(single_digits[0], dig_refs[0], cv2.TM_CCOEFF_NORMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_digit(single_digits[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
