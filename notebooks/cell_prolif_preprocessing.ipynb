{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import pytesseract\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define image paths & some other items we'll need\n",
    "\n",
    "Note the OpenCV variables in all-caps. These are just helpful variable names for values OpenCV uses internally. This style of all-caps is commonly found in C programming for enumeration data types (constants), and is used to help make programs easier to read and maintain.\n",
    "\n",
    "See https://docs.opencv.org/4.2.0/d4/d86/group__imgproc__filter.html for more info on enumerations related to image filtering in OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_ellipse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "kernel_cross = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [1 1 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(kernel_cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You'll need to fix the path to your images here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../t01/'\n",
    "apt_ref_path = 'apt_ref.tif'\n",
    "img_paths = glob(os.path.join(data_dir, '*.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_mask = Image.open(apt_ref_path)\n",
    "apt_ref_mask = np.asarray(apt_ref_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_c, _ = cv2.findContours(apt_ref_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(apt_ref_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref_c = apt_ref_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mask = np.zeros(apt_ref_mask.shape, dtype=np.uint8)\n",
    "cv2.drawContours(c_mask, [apt_ref_c], 0, 255, 1)\n",
    "plt.figure(figsize=(8, 8))\n",
    "_ = plt.imshow(c_mask, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a test image\n",
    "\n",
    "Load the first image in the list using the PIL library (the only usage of PIL we will need). Once loaded, I check the shape and min/max values to determine the number of channels in the image and the range of values (8-bit vs 16-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = img_paths[0]\n",
    "print(os.path.basename(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(img_path)\n",
    "img = np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, img.max(), img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img.min(), img.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see the file is a single channel grayscale image with 16-bit pixel values. I want to work with 8-bit pixel values, so we'll scale the values down. We could have simply cast the 16-bit array to 8-bit but these operations often will automatically normalize the min/max values in the data. We don't want to alter the data other than to scale it.\n",
    "\n",
    "First, we scale the 16-bit integers to an 8-bit range, but this creates floats. The floats are then cast to uint8.\n",
    "\n",
    "**I'm also flipping the image horizontally after the 8-bit conversion because I found the NumPy flip method altered the original 16-bit values (don't know why)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b = img / (2**8 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b.min(), img_8b.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b = img_8b.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b = cv2.flip(img_8b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_8b.min(), img_8b.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "_ = plt.imshow(img_8b, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply slight blur\n",
    "\n",
    "I found applying a slight blur here avoided a bunch of miniscule (1-5px) blobs in the subsequent thresholding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_blur = cv2.GaussianBlur(img_8b, (5, 5), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "_ = plt.imshow(img_blur, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.xlim(0, 256)\n",
    "plt.xticks(range(0, 257, 8))\n",
    "_ = plt.hist(img_8b.flatten(), bins=2**8 - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.xlim(0, 256)\n",
    "plt.xticks(range(0, 257, 8))\n",
    "_ = plt.hist(img_blur.flatten(), bins=2**8 - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll compare applying the adaptive threshold before and after the blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_edges = cv2.adaptiveThreshold(img_8b, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "_ = plt.imshow(img_edges, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above image shows the thresholding without the blur pre-processing. Note the small \"noise\"-like regions found. Below is using the slight blur image that avoids these regions. I kept the non-blur image in case we wanted to pre-filter out these small regions instead of blurring them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_edges_blur = cv2.adaptiveThreshold(img_blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "_ = plt.imshow(img_edges_blur, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we apply aggressive closing of the inverse mask (~img_edges). This is done to connect all the parts of the cross fiducials. As you see from above, the bright reflection in their centers causes them to be slightly broken into about 4 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_closed = cv2.morphologyEx(~img_edges_blur, cv2.MORPH_CLOSE, kernel_ellipse, iterations=1)\n",
    "edges_closed = cv2.morphologyEx(edges_closed, cv2.MORPH_CLOSE, kernel_cross, iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "_ = plt.imshow(edges_closed, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we find the contours as a simple list (no hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv2.findContours(\n",
    "    edges_closed,\n",
    "    cv2.RETR_LIST,\n",
    "    cv2.CHAIN_APPROX_SIMPLE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = cv2.cvtColor(img_8b, cv2.COLOR_GRAY2RGB)\n",
    "cv2.drawContours(new_img, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering out all the giant regions so we can make room to address the remaining non-fiducial regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all contours larger than ~625px\n",
    "max_px = 625\n",
    "\n",
    "small_contours = []\n",
    "\n",
    "for c in contours:\n",
    "    area = cv2.contourArea(c)\n",
    "    \n",
    "    if area > max_px:\n",
    "        continue\n",
    "    \n",
    "    small_contours.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = cv2.cvtColor(img_8b, cv2.COLOR_GRAY2RGB)\n",
    "cv2.drawContours(new_img, small_contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img[:, :], cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we check the fiducials are isolated. Notice the clusters of unwanted regions that are close to each other. To remove these we dilate the whole mask and re-apply a stricter filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiducials are relatively isolated\n",
    "# render remaining contours, dilate and re-find contours\n",
    "small_c_mask = np.zeros(img_8b.shape, dtype=np.uint8)\n",
    "_ = cv2.drawContours(small_c_mask, small_contours, -1, 255, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(small_c_mask, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_c_mask = cv2.morphologyEx(small_c_mask, cv2.MORPH_CLOSE, kernel_ellipse, iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(small_c_mask, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find contours\n",
    "contours, hierarchy = cv2.findContours(\n",
    "    small_c_mask,\n",
    "    cv2.RETR_LIST,\n",
    "    cv2.CHAIN_APPROX_SIMPLE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_px = 110\n",
    "max_px = 160\n",
    "max_aspect_ratio = 1.2\n",
    "\n",
    "small_contours2 = []\n",
    "c_centers = []\n",
    "\n",
    "for c in contours:\n",
    "    area = cv2.contourArea(c)\n",
    "    \n",
    "    # check area first, this will also filter out zero area contours (avoid div by 0)\n",
    "    if area > max_px or area < min_px:\n",
    "        continue\n",
    "    \n",
    "    c_min_rect = cv2.minAreaRect(c)\n",
    "    loc = c_min_rect[0]\n",
    "    (c_min_w, c_min_h) = c_min_rect[1]\n",
    "    c_angle = c_min_rect[2]\n",
    "    \n",
    "    aspect_ratio = c_min_h / c_min_w\n",
    "    \n",
    "    if aspect_ratio < 1:\n",
    "        aspect_ratio = 1. / aspect_ratio\n",
    "    \n",
    "    if aspect_ratio > max_aspect_ratio:\n",
    "        continue\n",
    "    \n",
    "    small_contours2.append(c)\n",
    "    c_centers.append(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(small_contours2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks good so far, however this is just on one test image. The above processing might need to be tweaked to work on many images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = cv2.cvtColor(img_8b, cv2.COLOR_GRAY2RGB)\n",
    "cv2.drawContours(new_img, small_contours2, -1, (0, 255, 0), 3)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we tackle the image rotation\n",
    "\n",
    "This is done by assigning fiducials to a row, then finding the slope of each row. I took the mean of the slopes and then created a transformation matrix for that rotation angle and applied it to our base 8-bit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearest_dists = []\n",
    "\n",
    "for loc in c_centers:\n",
    "    dists = cdist([loc], c_centers)[0]\n",
    "    dists.sort()\n",
    "    nearest_dists.append(dists[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_y = [cnt[1] for cnt in c_centers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we plot a histogram of the y-axis center locations\n",
    "\n",
    "**This or some variation could be used to QC that we have successfully isolated just the fiducials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h = img_8b.shape[1]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.xlim(0, img_h)\n",
    "plt.xticks(range(0, img_h, 100))\n",
    "_ = plt.hist(centers_y, bins=int(np.sqrt(img_h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "centers_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rows are separated by roughly 220px\n",
    "assigned_idx = []\n",
    "centers_y = np.array(centers_y)\n",
    "row_dist = 110\n",
    "rows = []\n",
    "\n",
    "for i, cy in enumerate(centers_y):\n",
    "    if i in assigned_idx:\n",
    "        continue\n",
    "    \n",
    "    row_min = cy - row_dist\n",
    "    row_max = cy + row_dist\n",
    "    \n",
    "    in_row = np.logical_and(centers_y > row_min, centers_y < row_max)\n",
    "    row_membership = np.where(in_row)\n",
    "    row_members = list(row_membership[0])\n",
    "    \n",
    "    rows.append(row_members)\n",
    "    assigned_idx.extend(row_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_centers = np.array(c_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the indexing for finding the y-coordinate of all center locations in the first row\n",
    "c_centers[rows[0]][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient, intercept, r_value, p_value, std_err = stats.linregress(c_centers[rows[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.degrees(np.arctan(gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_degs = []\n",
    "\n",
    "for r in rows:\n",
    "    gradient, intercept, r_value, p_value, std_err = stats.linregress(c_centers[r])\n",
    "    r_deg = np.degrees(np.arctan(gradient))\n",
    "    r_degs.append(r_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_degs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_deg_mean = np.mean(r_degs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_deg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = img_8b.shape\n",
    "\n",
    "rot_mat = cv2.getRotationMatrix2D((cols/2., rows/2.), r_deg_mean, 1)\n",
    "img_rot = cv2.warpAffine(img_8b, rot_mat, (cols, rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(img_rot, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I define a function to rotate a point around another point. This allows us to transform all the fiducial center locations to the rotated space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(point, origin=(0, 0), degrees=0):\n",
    "    angle = np.deg2rad(-degrees)\n",
    "    \n",
    "    ox, oy = origin\n",
    "    px, py = point\n",
    "\n",
    "    qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)\n",
    "    qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)\n",
    "    \n",
    "    return qx, qy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_c = rotate(c_centers[29], origin=(cols/2., rows/2.), degrees=r_deg_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_centers[29], rot_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rotate function works, so apply it to all fiducial centers. However, while we do this I calculate the bounding boxes for the regions of interest relative to the fiducials, e.g. the apartment row/col numbers. Additionally, I collect regions I wanted to use for applying the luminosity correction...but this didn't work well and the regions I selected are inside the apartment so wouldn't be ideal for later time points when they could be filled with cells :(  I have removed the uniformity correction code from here so you don't have to install my cv2-extras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = cv2.cvtColor(img_rot, cv2.COLOR_GRAY2RGB)\n",
    "row_text_regions = []\n",
    "uni_corr_regions = []\n",
    "apt_regions = []\n",
    "\n",
    "for c_center in c_centers:\n",
    "    rot_c = rotate(c_center, origin=(cols/2., rows/2.), degrees=r_deg_mean)\n",
    "    c_int_tup = tuple(np.round(rot_c).astype(np.int))\n",
    "    \n",
    "    # rect for non-uniformity samples\n",
    "    rect_vert1 = (c_int_tup[0] - 80, c_int_tup[1] - 50)\n",
    "    rect_vert2 = (c_int_tup[0] - 30, c_int_tup[1])\n",
    "    \n",
    "    uni_corr_regions.append(\n",
    "        [\n",
    "            rect_vert1,\n",
    "            (c_int_tup[0] - 30, c_int_tup[1] - 50),\n",
    "            rect_vert2,\n",
    "            (c_int_tup[0] - 80, c_int_tup[1])\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # rect for row number\n",
    "    row_rect_vert1 = (c_int_tup[0] - 10, c_int_tup[1] - 128)\n",
    "    row_rect_vert2 = (c_int_tup[0] + 40, c_int_tup[1] - 100)\n",
    "    \n",
    "    row_text_regions.append(\n",
    "        img_rot[c_int_tup[1] - 128:c_int_tup[1] - 100, c_int_tup[0] - 10:c_int_tup[0] + 40]\n",
    "    )\n",
    "    \n",
    "    # rect for col number\n",
    "    col_rect_vert1 = (c_int_tup[0] - 148, c_int_tup[1] - 30)\n",
    "    col_rect_vert2 = (c_int_tup[0] - 98, c_int_tup[1] - 2)\n",
    "    \n",
    "    # apt region\n",
    "    apt_offset_x = c_int_tup[0] - apt_ref_mask.shape[1] - 10\n",
    "    apt_offset_y = c_int_tup[1] - apt_ref_mask.shape[0] + 45\n",
    "    apt_c = apt_ref_c + [apt_offset_x, apt_offset_y]\n",
    "    \n",
    "    cv2.circle(new_img, c_int_tup, 5, (0, 255, 0), -1)\n",
    "    #cv2.rectangle(new_img, rect_vert1, rect_vert2, (0, 255, 0), 1)\n",
    "    cv2.rectangle(new_img, row_rect_vert1, row_rect_vert2, (0, 255, 0), 1)\n",
    "    cv2.rectangle(new_img, col_rect_vert1, col_rect_vert2, (0, 255, 0), 1)\n",
    "    cv2.drawContours(new_img, [apt_c], 0, (0, 255, 0), 1)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img[300:800, 300:800], cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(new_img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below is OCR related and is just me playing around...nothing worked satisfactorily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = r'--oem 3 --psm 8 outputbase digits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in row_text_regions:\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(pytesseract.image_to_string(r, config=custom_config))\n",
    "    plt.imshow(r, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_img = row_text_regions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_img_bilat = cv2.bilateralFilter(sub_img, 3, 5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img_bilat, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply automatic Canny edge detection using the computed median\n",
    "med = np.median(sub_img)\n",
    "sigma = 0.33\n",
    "lower = int(max(0, (1.0 - sigma) * med))\n",
    "upper = int(min(255, (1.0 + sigma) * med))\n",
    "\n",
    "sub_img_canny = cv2.Canny(sub_img, lower, upper, apertureSize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "plt.xlim(0, 256)\n",
    "plt.xticks(range(0, 257, 8))\n",
    "_ = plt.hist(sub_img.flatten(), bins=2**8 - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, sub_img_bkgd_mask = cv2.threshold(sub_img, med, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img_bkgd_mask, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img_canny, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_img_edges = cv2.adaptiveThreshold(sub_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img_edges, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_img_edges[0:3] = 255\n",
    "sub_img_edges[-3:] = 255\n",
    "sub_img_edges[:, 0:2] = 255\n",
    "sub_img_edges[:, -1:] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img_edges, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_img_edges = np.pad(sub_img_edges, 3, constant_values=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img_edges, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.image_to_string(sub_img_edges, config=custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_img_closed = cv2.morphologyEx(sub_img_edges, cv2.MORPH_OPEN, kernel_cross, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sub_img_closed, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.image_to_string(~sub_img_closed, config=custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2x.filter_contours_by_size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in row_text_regions:\n",
    "    r_pre = r\n",
    "    # r_pre = cv2.bilateralFilter(r, 3, 3, 15)\n",
    "\n",
    "    sub_img_edges = cv2.adaptiveThreshold(r_pre, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 0)\n",
    "    #sub_img_edges = cv2.morphologyEx(sub_img_edges, cv2.MORPH_OPEN, kernel_cross, iterations=1)\n",
    "    \n",
    "    #new_contours = cv2x.filter_contours_by_size(sub_img_edges, 9)\n",
    "    \n",
    "    #sub_img_edges = np.zeros(sub_img_edges.shape, dtype=np.uint8)\n",
    "    #_ = cv2.drawContours(sub_img_edges, new_contours, -1, 255, -1)\n",
    "    # sub_img_edges = ~sub_img_edges\n",
    "    \n",
    "#     sub_img_edges = cv2.morphologyEx(sub_img_edges, cv2.MORPH_DILATE, kernel_cross, iterations=1)\n",
    "    \n",
    "    sub_img_edges[0:3] = 255\n",
    "    sub_img_edges[-3:] = 255\n",
    "    sub_img_edges[:, 0:2] = 255\n",
    "    sub_img_edges[:, -1:] = 255\n",
    "    \n",
    "    sub_img_edges = np.pad(sub_img_edges, 5, constant_values=255)\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(pytesseract.image_to_string(sub_img_edges, config=custom_config))\n",
    "    plt.imshow(sub_img_edges, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
